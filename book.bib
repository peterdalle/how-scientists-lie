@article{john_measuring_2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  journal = {Psychological Science},
  volume = {23},
  number = {5},
  pages = {524--532},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611430953},
  langid = {english}
}

@misc{stefan_big_2022,
  title = {Big {{Little Lies}}: {{A Compendium}} and {{Simulation}} of p-{{Hacking Strategies}}},
  shorttitle = {Big {{Little Lies}}},
  author = {Stefan, Angelika and Sch{\"o}nbrodt, Felix},
  year = {2022},
  month = mar,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/xy2dk},
  abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is p-hacking. Typically, p-hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these p-hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of twelve p-hacking strategies based on an extensive literature review, identify factors that control their level of severity, and demonstrate their impact on false-positive rates using simulation studies. We also use our simulation results to evaluate several approaches that have been proposed to mitigate the influence of questionable research practices. Our results show that investigating p-hacking at the level of strategies can provide a better understanding of the process of p-hacking, as well as a broader basis for developing effective countermeasures. By making our analyses available through a Shiny app and R package, we facilitate future meta-scientific research aimed at investigating the ramifications of p-hacking across multiple strategies, and we hope to start a broader discussion about different manifestations of p-hacking in practice.},
  langid = {american}
}

@article{greenland_need_2017,
  title = {The Need for Cognitive Science in Methodology},
  author = {Greenland, Sander},
  year = {2017},
  month = jun,
  journal = {American Journal of Epidemiology},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwx259},
  langid = {english}
}

@article{benjamin_redefine_2018,
  title = {Redefine Statistical Significance},
  author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Bj{\"o}rn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and De Boeck, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Hua Ho, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munaf{\'o}, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Sch{\"o}nbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Van Zandt, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
  year = {2018},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {1},
  pages = {6--10},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0189-z},
  abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
  copyright = {2017 The Author(s)},
  langid = {english}
}

@article{lakens_justify_2018,
  title = {Justify Your Alpha},
  author = {Lakens, Daniel and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and {van Harmelen}, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and {de Oliveira}, Cilene Lino and {de Xivry}, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k{a}}tkowski, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {3},
  pages = {168--171},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  abstract = {In response to recommendations to redefine statistical significance to P {$\leq$} 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  copyright = {2018 The Publisher},
  langid = {english},
}

@book{research_responsible_1992,
  title = {Responsible {{Science}}: {{Ensuring}} the {{Integrity}} of the {{Research Process}}: {{Volume I}}},
  shorttitle = {Responsible {{Science}}},
  author = {{National Academy of Sciences (US), National Academy of Engineering (US) and Institute of Medicine (US) Panel on Scientific Responsibility and the Conduct of Research}},
  year = {1992},
  publisher = {{National Academies Press (US)}},
  address = {{Washington (DC)}},
  abstract = {Responsible Science is a comprehensive review of factors that influence the integrity of the research process. Volume I examines reports on the incidence of misconduct in science and reviews institutional and governmental efforts to handle cases of misconduct. The result of a two-year study by a panel of experts convened by the National Academy of Sciences, this book critically analyzes the impact of today's research environment on the traditional checks and balances that foster integrity in science. Responsible Science is a provocative examination of the role of educational efforts; research guidelines; and the contributions of individual scientists, mentors, and institutional officials in encouraging responsible research practices.},
  copyright = {Copyright \textcopyright{} 1992 by the National Academy of Sciences.},
  isbn = {978-0-309-04591-9},
  langid = {english},
  lccn = {NBK234523},
  pmid = {25121265}
}

@article{bright_why_2021,
  title = {Why {{Do Scientists Lie}}?},
  author = {Bright, Liam Kofi},
  year = {2021},
  month = may,
  journal = {Royal Institute of Philosophy Supplements},
  volume = {89},
  pages = {117--129},
  issn = {1358-2461, 1755-3555},
  doi = {10.1017/S1358246121000102},
  abstract = {Abstract             It's natural to think of scientists as truth seekers, people driven by an intense curiosity to understand the natural world. Yet this picture of scientists and scientific inquiry sits uncomfortably with the reality and prevalence of scientific fraud. If one wants to get at the truth about nature, why lie? Won't that just set inquiry back, as people pursue false leads? To understand why this occurs \textendash{} and what can be done about it \textendash{} we need to understand the social structures scientists work within, and how some of the institutions which enable science to be such a successful endeavour all things considered, also abet and encourage fraud.},
  langid = {english}
}

@techreport{simmons_21_2012,
  title = {A 21 {{Word Solution}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2012},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2160588},
  month = oct,
  number = {ID 2160588},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {One year after publishing "False-Positive Psychology," we propose a simple implementation of disclosure that requires but 21 words to achieve full transparency. This article is written in a casual tone. It includes phone-taken pictures of milk-jars and references to ice-cream and sardines.},
  langid = {english}
}

@article{gopalakrishna_prevalence_2022,
  title = {Prevalence of Questionable Research Practices, Research Misconduct and Their Potential Explanatory Factors: {{A}} Survey among Academic Researchers in {{The Netherlands}}},
  shorttitle = {Prevalence of Questionable Research Practices, Research Misconduct and Their Potential Explanatory Factors},
  author = {Gopalakrishna, Gowri and ter Riet, Gerben and Vink, Gerko and Stoop, Ineke and Wicherts, Jelte M. and Bouter, Lex M.},
  year = {2022},
  month = feb,
  journal = {PLOS ONE},
  volume = {17},
  number = {2},
  pages = {e0263023},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0263023},
  abstract = {Prevalence of research misconduct, questionable research practices (QRPs) and their associations with a range of explanatory factors has not been studied sufficiently among academic researchers. The National Survey on Research Integrity targeted all disciplinary fields and academic ranks in the Netherlands. It included questions about engagement in fabrication, falsification and 11 QRPs over the previous three years, and 12 explanatory factor scales. We ensured strict identity protection and used the randomized response method for questions on research misconduct. 6,813 respondents completed the survey. Prevalence of fabrication was 4.3\% (95\% CI: 2.9, 5.7) and of falsification 4.2\% (95\% CI: 2.8, 5.6). Prevalence of QRPs ranged from 0.6\% (95\% CI: 0.5, 0.9) to 17.5\% (95\% CI: 16.4, 18.7) with 51.3\% (95\% CI: 50.1, 52.5) of respondents engaging frequently in at least one QRP. Being a PhD candidate or junior researcher increased the odds of frequently engaging in at least one QRP, as did being male. Scientific norm subscription (odds ratio (OR) 0.79; 95\% CI: 0.63, 1.00) and perceived likelihood of detection by reviewers (OR 0.62, 95\% CI: 0.44, 0.88) were associated with engaging in less research misconduct. Publication pressure was associated with more often engaging in one or more QRPs frequently (OR 1.22, 95\% CI: 1.14, 1.30). We found higher prevalence of misconduct than earlier surveys. Our results suggest that greater emphasis on scientific norm subscription, strengthening reviewers in their role as gatekeepers of research quality and curbing the ``publish or perish'' incentive system promotes research integrity.},
  langid = {english}
}

@article{meehl_why_1990,
  title = {Why {{Summaries}} of {{Research}} on {{Psychological Theories}} Are {{Often Uninterpretable}}},
  author = {Meehl, Paul E.},
  year = {1990},
  month = feb,
  journal = {Psychological Reports},
  volume = {66},
  number = {1},
  pages = {195--244},
  issn = {0033-2941},
  doi = {10.2466/pr0.1990.66.1.195},
  abstract = {Null hypothesis testing of correlational predictions from weak substantive theories in soft psychology is subject to the influence of ten obfuscating factors whose effects are usually (1) sizeable, (2) opposed, (3) variable, and (4) unknown. The net epistemic effect of these ten obfuscating influences is that the usual research literature review is well-nigh uninterpretable. Major changes in graduate education, conduct of research, and editorial policy are proposed.},
  langid = {english}
}

@article{smaldino_natural_2016,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, E. and McElreath, Richard},
  year = {2016},
  journal = {Royal Society Open Science},
  volume = {3},
  number = {9},
  doi = {10.1098/rsos.160384},
  abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing\textemdash no deliberate cheating nor loafing\textemdash by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more `progeny,' such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.}
}

@article{de_groot_meaning_1956,
  title = {The {{Meaning}} of ``{{Significance}}'' for {{Different Types}} of {{Research}}},
  author = {{de Groot}, Adriaan. D.},
  year = {1956},
  journal = {Nederlands Tijdschrift voor de Psychologie en Haar Grensgebieden},
  abstract = {Adrianus Dingeman de Groot (1914\textendash 2006) was one of the most influential Dutch psychologists. He became famous for his work ``Thought and Choice in Chess'', but his main contribution was methodological \textemdash{} De Groot cofounded the Department of Psychological Methods at the University of Amsterdam (together with R. F. van Naerssen), founded one of the leading testing and assessment companies (CITO), and wrote the monograph ``Methodology'' that centers on the empirical-scientific cycle: observation\textendash induction\textendash{} deduction\textendash testing\textendash evaluation. Here we translate one of De Groot's early articles, published in 1956 in the Dutch journal Nederlands Tijdschrift voor de Psychologie en Haar Grensgebieden. This article is more topical now than it was almost 60 years ago. De Groot stresses the difference between exploratory and confirmatory (``hypothesis testing'') research and argues that statistical inference is only sensible for the latter: ``One `is allowed' to apply statistical tests in exploratory research, just as long as one realizes that they do not have evidential impact''. De Groot may have also been one of the first psychologists to argue explicitly for preregistration of experiments and the associated plan of statistical analysis. The appendix provides annotations that connect De Groot's arguments to the current-day debate on transparency and reproducibility in psychological science.}
}

@book{hacking_social_1999,
  title = {The Social Construction of What?},
  author = {Hacking, Ian},
  year = {1999},
  publisher = {{Harvard University Press}},
  address = {{Cambridge, Mass.}},
  isbn = {0-674-81200-X (alk. paper) (inb.)}
}

@article{rosnow_statistical_1989,
  title = {Statistical Procedures and the Justification of Knowledge in Psychological Science},
  author = {Rosnow, Ralph L. and Rosenthal, Robert},
  year = {1989},
  journal = {American Psychologist},
  volume = {44},
  number = {10},
  pages = {1276--1284},
  issn = {1935-990X 0003-066X},
  doi = {10.1037/0003-066X.44.10.1276},
  abstract = {Justification, in the vernacular language of philosophy of science, refers to the evaluation, defense, and confirmation of claims of truth. In this article, we examine some aspects of the rhetoric of justification, which in part draws on statistical data analysis to shore up facts and inductive inferences. There are a number of problems of methodological spirit and substance that in the past have been resistant to attempts to correct them. The major problems are discussed, and readers are reminded of ways to clear away these obstacles to justification.},
  langid = {english}
}

@article{kerr_harking_1998,
  title = {{{HARKing}}: Hypothesizing after the Results Are Known},
  shorttitle = {{{HARKing}}},
  author = {Kerr, N. L.},
  year = {1998},
  journal = {Personality and Social Psychology Review},
  volume = {2},
  number = {3},
  pages = {196--217},
  issn = {1088-8683},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  langid = {english},
  pmid = {15647155}
}

@article{hollenbeck_harking_2017,
  title = {Harking, {{Sharking}}, and {{Tharking}}: {{Making}} the {{Case}} for {{Post Hoc Analysis}} of {{Scientific Data}}},
  shorttitle = {Harking, {{Sharking}}, and {{Tharking}}},
  author = {Hollenbeck, John R. and Wright, Patrick M.},
  year = {2017},
  month = jan,
  journal = {Journal of Management},
  volume = {43},
  number = {1},
  pages = {5--18},
  issn = {0149-2063},
  doi = {10.1177/0149206316679487},
  abstract = {In this editorial we discuss the problems associated with HARKing (Hypothesizing After Results Are Known) and draw a distinction between Sharking (Secretly HARKing in the Introduction section) and Tharking (Transparently HARKing in the Discussion section). Although there is never any justification for the process of Sharking, we argue that Tharking can promote the effectiveness and efficiency of both scientific inquiry and cumulative knowledge creation. We argue that the discussion sections of all empirical papers should include a subsection that reports post hoc exploratory data analysis. We explain how authors, reviewers, and editors can best leverage post hoc analyses in the spirit of scientific discovery in a way that does not bias parameter estimates and recognizes the lack of definitiveness associated with any single study or any single replication. We also discuss why the failure to Thark in high-stakes contexts where data is scarce and costly may also be unethical.},
  langid = {english}
}

@article{lishner_harking_2021,
  title = {{{HARKing}}: {{Conceptualizations}}, Harms, and Two Fundamental Remedies.},
  shorttitle = {{{HARKing}}},
  author = {Lishner, David A.},
  year = {2021},
  month = apr,
  journal = {Journal of Theoretical and Philosophical Psychology},
  issn = {2151-3341, 1068-8471},
  doi = {10.1037/teo0000182},
  langid = {english}
}

@article{rubin_costs_nodate,
  title = {The {{Costs}} of {{HARKing}}},
  author = {Rubin, Mark},
  journal = {The British Journal for the Philosophy of Science},
  doi = {10.1093/bjps/axz050},
  abstract = {Abstract.  Kerr ([1998]) coined the term `HARKing' to refer to the practice of `hypothesizing after the results are known'. This questionable research practice},
  langid = {english}
}

@article{rubin_when_2017,
  title = {When {{Does HARKing Hurt}}? {{Identifying When Different Types}} of {{Undisclosed Post Hoc Hypothesizing Harm Scientific Progress}}},
  shorttitle = {When {{Does HARKing Hurt}}?},
  author = {Rubin, Mark},
  year = {2017},
  month = dec,
  journal = {Review of General Psychology},
  volume = {21},
  number = {4},
  pages = {308--320},
  issn = {1089-2680},
  doi = {10.1037/gpr0000128},
  abstract = {Hypothesizing after the results are known, or HARKing, occurs when researchers check their research results and then add or remove hypotheses on the basis of those results without acknowledging this process in their research report (Kerr, 1998). In the present article, I discuss 3 forms of HARKing: (a) using current results to construct post hoc hypotheses that are then reported as if they were a priori hypotheses; (b) retrieving hypotheses from a post hoc literature search and reporting them as a priori hypotheses; and (c) failing to report a priori hypotheses that are unsupported by the current results. These 3 types of HARKing are often characterized as being bad for science and a potential cause of the current replication crisis. In the present article, I use insights from the philosophy of science to present a more nuanced view. Specifically, I identify the conditions under which each of these 3 types of HARKing is most and least likely to be bad for science. I conclude with a brief discussion about the ethics of each type of HARKing.},
  langid = {english}
}

@article{agnoli_questionable_2017,
  title = {Questionable Research Practices among Italian Research Psychologists},
  author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Albiero, Paolo and Cubelli, Roberto},
  year = {2017},
  month = mar,
  journal = {PLOS ONE},
  volume = {12},
  number = {3},
  pages = {e0172792},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0172792},
  abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88\%) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.}
}

@article{fiedler_questionable_2016,
  title = {Questionable {{Research Practices Revisited}}},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  year = {2016},
  month = jan,
  journal = {Social Psychological and Personality Science},
  volume = {7},
  number = {1},
  pages = {45--52},
  issn = {1948-5506},
  doi = {10.1177/1948550615612150},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  langid = {english}
}

@article{fraser_questionable_2018,
  title = {Questionable Research Practices in Ecology and Evolution},
  author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
  year = {2018},
  month = jul,
  journal = {PLOS ONE},
  volume = {13},
  number = {7},
  pages = {e0200303},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0200303},
  abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
  langid = {english}
}

@article{krishna_questionable_2018,
  title = {Questionable Research Practices in Student Final Theses \textendash{} {{Prevalence}}, Attitudes, and the Role of the Supervisor's Perceived Attitudes},
  author = {Krishna, Anand and Peter, Sebastian M.},
  year = {2018},
  month = aug,
  journal = {PLOS ONE},
  volume = {13},
  number = {8},
  pages = {e0203470},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0203470},
  abstract = {Although questionable research practices (QRPs) and p-hacking have received attention in recent years, little research has focused on their prevalence and acceptance in students. Students are the researchers of the future and will represent the field in the future. Therefore, they should not be learning to use and accept QRPs, which would reduce their ability to produce and evaluate meaningful research. 207 psychology students and fresh graduates provided self-report data on the prevalence and predictors of QRPs. Attitudes towards QRPs, belief that significant results constitute better science or lead to better grades, motivation, and stress levels were predictors. Furthermore, we assessed perceived supervisor attitudes towards QRPs as an important predictive factor. The results were in line with estimates of QRP prevalence from academia. The best predictor of QRP use was students' QRP attitudes. Perceived supervisor attitudes exerted both a direct and indirect effect via student attitudes. Motivation to write a good thesis was a protective factor, whereas stress had no effect. Students in this sample did not subscribe to beliefs that significant results were better for science or their grades. Such beliefs further did not impact QRP attitudes or use in this sample. Finally, students engaged in more QRPs pertaining to reporting and analysis than those pertaining to study design. We conclude that supervisors have an important function in shaping students' attitudes towards QRPs and can improve their research practices by motivating them well. Furthermore, this research provides some impetus towards identifying predictors of QRP use in academia.},
  langid = {english}
}

@article{motyl_state_2017,
  title = {The State of Social and Personality Science: {{Rotten}} to the Core, Not so Bad, Getting Better, or Getting Worse?},
  shorttitle = {The State of Social and Personality Science},
  author = {Motyl, Matt and Demos, Alexander P. and Carsel, Timothy S. and Hanson, Brittany E. and Melton, Zachary J. and Mueller, Allison B. and Prims, J. P. and Sun, Jiaqing and Washburn, Anthony N. and Wong, Kendal M. and Yantis, Caitlyn and Skitka, Linda J.},
  year = {2017},
  month = jul,
  journal = {Journal of Personality and Social Psychology},
  volume = {113},
  number = {1},
  pages = {34--58},
  issn = {1939-1315},
  doi = {10.1037/pspa0000084},
  abstract = {The scientific quality of social and personality psychology has been debated at great length in recent years. Despite research on the prevalence of Questionable Research Practices (QRPs) and the replicability of particular findings, the impact of the current discussion on research practices is unknown. The current studies examine whether and how practices have changed, if at all, over the last 10 years. In Study 1, we surveyed 1,166 social and personality psychologists about how the current debate has affected their perceptions of their own and the field's research practices. In Study 2, we coded the research practices and critical test statistics from social and personality psychology articles published in 2003-2004 and 2013-2014. Together, these studies suggest that (a) perceptions of the current state of the field are more pessimistic than optimistic; (b) the discussion has increased researchers' intentions to avoid QRPs and adopt proposed best practices, (c) the estimated replicability of research published in 2003-2004 may not be as bad as many feared, and (d) research published in 2013-2014 shows some improvement over research published in 2003-2004, a result that suggests the field is evolving in a positive direction. (PsycINFO Database Record},
  langid = {english},
  pmid = {28447837}
}

@article{banks_questions_2016,
  title = {Questions {{About Questionable Research Practices}} in the {{Field}} of {{Management}}: {{A Guest Commentary}}},
  shorttitle = {Questions {{About Questionable Research Practices}} in the {{Field}} of {{Management}}},
  author = {Banks, George C. and O'Boyle, Ernest H. and Pollack, Jeffrey M. and White, Charles D. and Batchelor, John H. and Whelpley, Christopher E. and Abston, Kristie A. and Bennett, Andrew A. and Adkins, Cheryl L.},
  year = {2016},
  month = jan,
  journal = {Journal of Management},
  volume = {42},
  number = {1},
  pages = {5--20},
  issn = {0149-2063},
  doi = {10.1177/0149206315619011},
  abstract = {The discussion regarding questionable research practices (QRPs) in management as well as the broader natural and social sciences has increased substantially in recent years. Despite the attention, questions remain regarding research norms and the implications for both theoretical and practical advancements. The aim of the current article is to address these issues in a question-and-answer format while drawing upon both past research and the results of a series of new studies conducted using a mixed-methods design. Our goal is to encourage a systematic, collegial, and constructive dialogue regarding QRPs in management research.}
}

@article{bosco_harkings_2016,
  title = {{{HARKing}}'s {{Threat}} to {{Organizational Research}}: {{Evidence From Primary}} and {{Meta-Analytic Sources}}},
  shorttitle = {{{HARKing}}'s {{Threat}} to {{Organizational Research}}},
  author = {Bosco, Frank A. and Aguinis, Herman and Field, James G. and Pierce, Charles A. and Dalton, Dan R.},
  year = {2016},
  journal = {Personnel Psychology},
  volume = {69},
  number = {3},
  pages = {709--750},
  issn = {1744-6570},
  doi = {10.1111/peps.12111},
  abstract = {We assessed presumed consequences of hypothesizing after results are known (HARKing) by contrasting hypothesized versus nonhypothesized effect sizes among 10 common relations in organizational behavior, human resource management, and industrial and organizational psychology research. In Study 1, we analyzed 247 correlations representing 9 relations with individual performance in 136 articles published in Journal of Applied Psychology and Personnel Psychology and provide evidence that correlations are significantly larger when hypothesized compared to nonhypothesized. In Study 2, we analyzed 281 effect sizes from a meta-analysis on the job satisfaction\textendash job performance relation and provide evidence that correlations are significantly larger when hypothesized compared to nonhypothesized. In addition, in Study 2, we documented that hypothesized variable pairs are more likely to be mentioned in article titles or abstracts. We also ruled out 13 alternative explanations to the presumed HARKing effect pertaining to methodological (e.g., unreliability, publication year, research setting, research design, measure contextualization, publication source) and substantive (e.g., predictor\textendash performance pair, performance measure, satisfaction measure, occupation, job/task complexity) issues. Our results suggest that HARKing seems to pose a threat to research results, substantive conclusions, and practical applications. We offer recommended solutions to the HARKing threat.},
  langid = {english}
}

@article{ioannidis_why_2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLoS Med},
  volume = {2},
  number = {8},
  pages = {e124},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Published research findings are sometimes refuted by subsequent evidence, says Ioannidis, with ensuing confusion and disappointment.}
}

@article{nuijten_prevalence_2016,
  title = {The Prevalence of Statistical Reporting Errors in Psychology (1985\textendash 2013)},
  author = {Nuijten, Mich{\`e}le B. and Hartgerink, Chris H. J. and {van Assen}, Marcel A. L. M. and Epskamp, Sacha and Wicherts, Jelte M.},
  year = {2016},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {48},
  number = {4},
  pages = {1205--1226},
  issn = {1554-3528},
  doi = {10.3758/s13428-015-0664-2},
  langid = {english}
}

@article{matthes_questionable_2015,
  title = {Questionable {{Research Practices}} in {{Experimental Communication Research}}: {{A Systematic Analysis From}} 1980 to 2013},
  shorttitle = {Questionable {{Research Practices}} in {{Experimental Communication Research}}},
  author = {Matthes, J{\"o}rg and Marquart, Franziska and Naderer, Brigitte and Arendt, Florian and Schmuck, Desir{\'e}e and Adam, Karoline},
  year = {2015},
  month = oct,
  journal = {Communication Methods and Measures},
  volume = {9},
  number = {4},
  pages = {193--207},
  issn = {1931-2458},
  doi = {10.1080/19312458.2015.1096334},
  abstract = {Questionable research practices (QRPs) pose a major threat to any scientific discipline. This article analyzes QRPs with a content analysis of more than three decades of published experimental research in four flagship communication journals: Journal of Communication, Communication Research, Journalism \& Mass Communication Quarterly, and Media Psychology. Findings reveal indications of small and insufficiently justified sample sizes, a lack of reported effect sizes, an indiscriminate removal of cases and items, an increasing inflation of p-values directly below p {$<$} .05, and a rising share of verified (as opposed to falsified) hypotheses. Implications for authors, reviewers, and editors are discussed.}
}

@article{simmons_false-positive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  langid = {english}
}

@article{lakens_performing_2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses: {{Sequential}} Analyses},
  shorttitle = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Daniel},
  year = {2014},
  month = dec,
  journal = {European Journal of Social Psychology},
  volume = {44},
  number = {7},
  pages = {701--710},
  issn = {00462772},
  doi = {10.1002/ejsp.2023},
  abstract = {Running studies with high statistical power, while effect size estimates in psychology are often inaccurate, leads to a practical challenge when designing an experiment. This challenge can be addressed by performing sequential analyses while the data collection is still in progress. At an interim analysis, data collection can be stopped whenever the results are convincing enough to conclude that an effect is present, more data can be collected, or the study can be terminated whenever it is extremely unlikely that the predicted effect will be observed if data collection would be continued. Such interim analyses can be performed while controlling the Type 1 error rate. Sequential analyses can greatly improve the efficiency with which data are collected. Additional flexibility is provided by adaptive designs where sample sizes are increased on the basis of the observed effect size. The need for pre-registration, ways to prevent experimenter bias, and a comparison between Bayesian approaches and null-hypothesis significance testing (NHST) are discussed. Sequential analyses, which are widely used in large-scale medical trials, provide an efficient way to perform high-powered informative experiments. I hope this introduction will provide a practical primer that allows researchers to incorporate sequential analyses in their research.},
  langid = {english}
}
